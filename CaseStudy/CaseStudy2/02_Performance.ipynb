{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fe35ca8-3554-48a3-84f2-5ce408dc2750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Loading: GPT_41.csv\n",
      "üîó Loading: 00_GPT_41_finetune_1.csv\n",
      "üîó Loading: 00_GPT_41_finetune_2.csv\n",
      "üîó Loading: 00_GPT_41_finetune_3.csv\n",
      "üîó Loading: 00_GPT_41_finetune_4.csv\n",
      "üîó Loading: 00_GPT_41_finetune_5.csv\n",
      "üîó Loading: GPT_5_nano_nr.csv\n",
      "üîó Loading: GPT_5_nano_r.csv\n",
      "üîó Loading: GPT_5_nr.csv\n",
      "üîó Loading: GPT_5_r.csv\n",
      "\n",
      "‚úÖ Merged file saved to: Result/01_Result_All.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Folder path (adjust if needed)\n",
    "data_dir = Path(\"Result\")  # or directly Path(\"/mnt/data\")\n",
    "\n",
    "# List of CSV files to merge (modify if file names differ)\n",
    "files_to_merge = [\n",
    "    \"GPT_41.csv\",\n",
    "    \"00_GPT_41_finetune_1.csv\",\n",
    "    \"00_GPT_41_finetune_2.csv\",\n",
    "    \"00_GPT_41_finetune_3.csv\",\n",
    "    \"00_GPT_41_finetune_4.csv\",\n",
    "    \"00_GPT_41_finetune_5.csv\",\n",
    "    \"GPT_5_nano_nr.csv\",\n",
    "    \"GPT_5_nano_r.csv\",\n",
    "    \"GPT_5_nr.csv\",\n",
    "    \"GPT_5_r.csv\",\n",
    "]\n",
    "\n",
    "# Load and merge\n",
    "merged_df = None\n",
    "\n",
    "for file in files_to_merge:\n",
    "    file_path = data_dir / file\n",
    "    if file_path.exists():\n",
    "        print(f\"üîó Loading: {file}\")\n",
    "        temp_df = pd.read_csv(file_path)\n",
    "\n",
    "        # Ensure consistent row alignment using index\n",
    "        if merged_df is None:\n",
    "            merged_df = temp_df.copy()\n",
    "        else:\n",
    "            merged_df = pd.concat([merged_df, temp_df], axis=1)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Skipped (not found): {file}\")\n",
    "\n",
    "# Save merged file\n",
    "output_path = data_dir / \"01_Result_All.csv\"\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"\\n‚úÖ Merged file saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2cee00-d546-43f8-ae50-97326287cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s2/c9qz20w14xq_8kbmpx4z114m0000gn/T/ipykernel_18391/1915098638.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  complete_results_df = pd.concat([complete_results_df, pd.DataFrame({\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Result/01_Result_All.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define the ground truth columns\n",
    "ground_truth_cols = ['Q3_1', 'Q3_2', 'Q3_3', 'Q3_4', 'Q3_5', 'Q3_6', 'Q3_7', 'Q3_8']\n",
    "\n",
    "# Initialize a dataframe to store the results\n",
    "complete_results_df = pd.DataFrame(columns=['Theme', 'Model', 'Accuracy', 'F1 Score', 'Precision', 'Recall'])\n",
    "\n",
    "# Model groupings with their corresponding columns\n",
    "complete_model_mapping = {\n",
    "    'GPT_41': [f'GPT_41_{i}' for i in range(1, 9)],\n",
    "    'GPT_41_finetune_1': [f'GPT_41_finetune_1_{i}' for i in range(1, 9)],\n",
    "    'GPT_41_finetune_2': [f'GPT_41_finetune_2_{i}' for i in range(1, 9)],\n",
    "    'GPT_41_finetune_3': [f'GPT_41_finetune_3_{i}' for i in range(1, 9)],\n",
    "    'GPT_41_finetune_4': [f'GPT_41_finetune_4_{i}' for i in range(1, 9)],\n",
    "    'GPT_41_finetune_5': [f'GPT_41_finetune_5_{i}' for i in range(1, 9)],\n",
    "    'GPT_5_r': [f'GPT_5_r_{i}' for i in range(1, 9)],\n",
    "    'GPT_5_nr': [f'GPT_5_nr_{i}' for i in range(1, 9)],\n",
    "    'GPT_5_nano_r': [f'GPT_5_nano_r_{i}' for i in range(1, 9)],\n",
    "    'GPT_5_nano_nr': [f'GPT_5_nano_nr_{i}' for i in range(1, 9)]\n",
    "\n",
    "}\n",
    "\n",
    "# Evaluate performance for each model and theme\n",
    "for model_name, columns in complete_model_mapping.items():\n",
    "    for idx, theme in enumerate(ground_truth_cols):\n",
    "        if idx < len(columns):\n",
    "            accuracy = accuracy_score(df[theme], df[columns[idx]])\n",
    "            f1 = f1_score(df[theme], df[columns[idx]], average='macro')\n",
    "            precision = precision_score(df[theme], df[columns[idx]], average='macro')\n",
    "            recall = recall_score(df[theme], df[columns[idx]], average='macro')\n",
    "\n",
    "            # Append the results to the dataframe\n",
    "            complete_results_df = pd.concat([complete_results_df, pd.DataFrame({\n",
    "                'Theme': [theme],\n",
    "                'Model': [model_name],\n",
    "                'Accuracy': [accuracy],\n",
    "                'F1 Score': [f1],\n",
    "                'Precision': [precision],\n",
    "                'Recall': [recall]\n",
    "            })], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c141d44-5d84-411f-b0a8-d2a3788cf8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Theme</th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q3_1</td>\n",
       "      <td>GPT_41</td>\n",
       "      <td>0.901257</td>\n",
       "      <td>0.664182</td>\n",
       "      <td>0.677278</td>\n",
       "      <td>0.653508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q3_2</td>\n",
       "      <td>GPT_41</td>\n",
       "      <td>0.892280</td>\n",
       "      <td>0.773945</td>\n",
       "      <td>0.768180</td>\n",
       "      <td>0.780138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q3_3</td>\n",
       "      <td>GPT_41</td>\n",
       "      <td>0.971275</td>\n",
       "      <td>0.810705</td>\n",
       "      <td>0.756429</td>\n",
       "      <td>0.899728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q3_4</td>\n",
       "      <td>GPT_41</td>\n",
       "      <td>0.815081</td>\n",
       "      <td>0.716917</td>\n",
       "      <td>0.695403</td>\n",
       "      <td>0.762821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q3_5</td>\n",
       "      <td>GPT_41</td>\n",
       "      <td>0.910233</td>\n",
       "      <td>0.608452</td>\n",
       "      <td>0.579608</td>\n",
       "      <td>0.741509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Q3_4</td>\n",
       "      <td>GPT_5_nano_nr</td>\n",
       "      <td>0.834829</td>\n",
       "      <td>0.503565</td>\n",
       "      <td>0.670475</td>\n",
       "      <td>0.521798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>Q3_5</td>\n",
       "      <td>GPT_5_nano_nr</td>\n",
       "      <td>0.973070</td>\n",
       "      <td>0.551987</td>\n",
       "      <td>0.986511</td>\n",
       "      <td>0.531250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>Q3_6</td>\n",
       "      <td>GPT_5_nano_nr</td>\n",
       "      <td>0.608618</td>\n",
       "      <td>0.608162</td>\n",
       "      <td>0.673024</td>\n",
       "      <td>0.665354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>Q3_7</td>\n",
       "      <td>GPT_5_nano_nr</td>\n",
       "      <td>0.619390</td>\n",
       "      <td>0.541969</td>\n",
       "      <td>0.576835</td>\n",
       "      <td>0.553301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>Q3_8</td>\n",
       "      <td>GPT_5_nano_nr</td>\n",
       "      <td>0.928187</td>\n",
       "      <td>0.564572</td>\n",
       "      <td>0.586992</td>\n",
       "      <td>0.553384</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Theme          Model  Accuracy  F1 Score  Precision    Recall\n",
       "0   Q3_1         GPT_41  0.901257  0.664182   0.677278  0.653508\n",
       "1   Q3_2         GPT_41  0.892280  0.773945   0.768180  0.780138\n",
       "2   Q3_3         GPT_41  0.971275  0.810705   0.756429  0.899728\n",
       "3   Q3_4         GPT_41  0.815081  0.716917   0.695403  0.762821\n",
       "4   Q3_5         GPT_41  0.910233  0.608452   0.579608  0.741509\n",
       "..   ...            ...       ...       ...        ...       ...\n",
       "75  Q3_4  GPT_5_nano_nr  0.834829  0.503565   0.670475  0.521798\n",
       "76  Q3_5  GPT_5_nano_nr  0.973070  0.551987   0.986511  0.531250\n",
       "77  Q3_6  GPT_5_nano_nr  0.608618  0.608162   0.673024  0.665354\n",
       "78  Q3_7  GPT_5_nano_nr  0.619390  0.541969   0.576835  0.553301\n",
       "79  Q3_8  GPT_5_nano_nr  0.928187  0.564572   0.586992  0.553384\n",
       "\n",
       "[80 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbb0b7e-29a8-4ac0-9d3b-16e5f77d7b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results_df.to_csv(\"Result/02_performance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac556da6-81f8-42b3-9c1a-dbb095773ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa084ba3-bdeb-4414-be69-668a57a7c232",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
